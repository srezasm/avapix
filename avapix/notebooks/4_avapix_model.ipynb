{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_model import ValidationModel\n",
    "from settings import *\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "from avapix.avapix_model import AvapixModel\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print (f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedFacesDataset(Dataset):\n",
    "    def __init__(self, num_samples) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.random_lengths = list(range(128))\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        curr_rand_len = random.choice(self.random_lengths)\n",
    "\n",
    "        curr_image_tensor = np.random.randint(0, 256, (8, 8, 3))\n",
    "        curr_image_tensor = torch.tensor(curr_image_tensor, device=DEVICE, dtype=torch.float)\n",
    "\n",
    "        random_seed = random.randint(0, 256)\n",
    "\n",
    "        output_img = utils.generate_input_v1(curr_image_tensor,\n",
    "                                             random_seed,\n",
    "                                             curr_rand_len)\n",
    "        output_img = output_img.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "\n",
    "        return output_img / 255\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "import settings\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "importlib.reload(settings)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(200_000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss(0.000001)\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/15 - Loss: 72.1028\t\tEpoch 002/15 - Loss: 0.1968\t\t\n",
      "Epoch 003/15 - Loss: 0.0725\t\tEpoch 004/15 - Loss: 0.0588\t\t\n",
      "Epoch 005/15 - Loss: 0.0574\t\tEpoch 006/15 - Loss: 0.0569\t\t\n",
      "Epoch 007/15 - Loss: 0.0566\t\tEpoch 008/15 - Loss: 0.0564\t\t\n",
      "Epoch 009/15 - Loss: 0.0562\t\tEpoch 010/15 - Loss: 0.0561\t\t\n",
      "Epoch 011/15 - Loss: 0.0560\t\tEpoch 012/15 - Loss: 0.0559\t\t\n",
      "Epoch 013/15 - Loss: 0.0558\t\tEpoch 014/15 - Loss: 0.0558\t\t\n",
      "Epoch 015/15 - Loss: 0.0557\t\tTraining complete.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "_ = 1\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_imgs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_imgs)\n",
    "\n",
    "        face_loss = loss_model(outputs)\n",
    "        loss = criterion(face_loss, batch_imgs, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        img = utils.img_tensor_to_numpy(outputs[0]) * 255\n",
    "        img = img.astype(np.uint8)\n",
    "        Image.fromarray(img).save(f'./train_outputs/{_:06d}.png')\n",
    "        _ += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\", end='\\t\\t')\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), AVAPIX_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.embed_raw_img_v1('bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh', 10).to(DEVICE) / 255\n",
    "_ = _.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "_ = _.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_ = model(_)\n",
    "utils.show_model_output_image(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = _ != 0\n",
    "i_[mask] = _[mask]\n",
    "\n",
    "utils.show_model_output_image(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.extract_text(i_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
