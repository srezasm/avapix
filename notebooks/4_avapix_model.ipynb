{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_model import ValidationModel\n",
    "from settings import *\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "from avapix.avapix_model import AvapixModel\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print (f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedFacesDataset(Dataset):\n",
    "    def __init__(self, num_samples) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.random_lengths = list(range(128))\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        curr_rand_len = random.choice(self.random_lengths)\n",
    "\n",
    "        curr_image_tensor = np.random.randint(0, 256, (8, 8, 3))\n",
    "        curr_image_tensor = torch.tensor(curr_image_tensor, device=DEVICE, dtype=torch.float)\n",
    "        output_img = utils.generate_input_v1(curr_image_tensor,\n",
    "                                             DEFAULT_RANDOM_SEED,\n",
    "                                             curr_rand_len)\n",
    "        output_img = output_img.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "\n",
    "        return output_img / 255\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "import settings\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "importlib.reload(settings)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(50_000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss(0.000001)\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srezas/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/15 - Loss: 268.7749\n",
      "Epoch 002/15 - Loss: 1.1134\n",
      "Epoch 003/15 - Loss: 0.5365\n",
      "Epoch 004/15 - Loss: 0.3267\n",
      "Epoch 005/15 - Loss: 0.2680\n",
      "Epoch 006/15 - Loss: 0.2216\n",
      "Epoch 007/15 - Loss: 0.1591\n",
      "Epoch 008/15 - Loss: 0.1042\n",
      "Epoch 009/15 - Loss: 0.0791\n",
      "Epoch 010/15 - Loss: 0.0662\n",
      "Epoch 011/15 - Loss: 0.0606\n",
      "Epoch 012/15 - Loss: 0.0583\n",
      "Epoch 013/15 - Loss: 0.0573\n",
      "Epoch 014/15 - Loss: 0.0566\n",
      "Epoch 015/15 - Loss: 0.0562\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "_ = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_imgs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_imgs)\n",
    "\n",
    "        face_loss = loss_model(outputs)\n",
    "        loss = criterion(face_loss, batch_imgs, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # for i, tensor_img in enumerate(outputs):\n",
    "    #     img = img_tensor_to_numpy(tensor_img) * 255\n",
    "    #     img = img.astype(np.uint8)\n",
    "    #     Image.fromarray(img).save(f'./train_outputs/{_}_{i}.png')\n",
    "    # _ += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    # if epoch % 10 == 9:\n",
    "    #     print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\", end='\\t')\n",
    "    #     if (epoch + 1) % 20 == 0:\n",
    "    #         print()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), AVAPIX_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "import validation_model\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "importlib.reload(validation_model)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils\n",
    "from validation_model import ValidationModel\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "GEN_PER_IMAGE = 10\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(100000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss()\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)\n",
    "\n",
    "for batch_imgs in dataloader:\n",
    "    outputs = model(batch_imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_model_output_image(outputs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.embed_raw_img_v1('bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh', 42).to(DEVICE) / 255\n",
    "_ = _.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "_ = _.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_ = model(_)\n",
    "utils.show_model_output_image(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = _ != 0\n",
    "i_[mask] = _[mask]\n",
    "\n",
    "utils.show_model_output_image(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.extract_text(i_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(960.)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(5, 3, 8, 8)\n",
    "t2 = torch.zeros(5, 3, 8, 8)\n",
    "\n",
    "mask = t1 > 0.5\n",
    "result = torch.zeros_like(t2)\n",
    "result[mask] = t2[mask]\n",
    "\n",
    "(result - t1).abs().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
