{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_model import ValidationModel\n",
    "from settings import *\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "from avapix.avapix_model import AvapixModel\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print (f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedFacesDataset(Dataset):\n",
    "    def __init__(self, num_samples) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.random_lengths = list(range(128))\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        curr_rand_len = random.choice(self.random_lengths)\n",
    "\n",
    "        curr_image_tensor = np.random.randint(0, 256, (8, 8, 3))\n",
    "        curr_image_tensor = torch.tensor(curr_image_tensor, device=DEVICE, dtype=torch.float)\n",
    "        output_img = utils.generate_input_v1(curr_image_tensor,\n",
    "                                             DEFAULT_RANDOM_SEED,\n",
    "                                             curr_rand_len)\n",
    "        output_img = output_img.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "\n",
    "        return output_img / 255\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(50_000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss(0.000001)\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/15 - Loss: 7.8098\n",
      "Epoch 002/15 - Loss: 4.0664\n",
      "Epoch 003/15 - Loss: 3.4634\n",
      "Epoch 004/15 - Loss: 3.1719\n",
      "Epoch 005/15 - Loss: 2.9691\n",
      "Epoch 006/15 - Loss: 2.8602\n",
      "Epoch 007/15 - Loss: 2.8128\n",
      "Epoch 008/15 - Loss: 2.7981\n",
      "Epoch 009/15 - Loss: 2.7920\n",
      "Epoch 010/15 - Loss: 2.7890\n",
      "Epoch 011/15 - Loss: 2.7873\n",
      "Epoch 012/15 - Loss: 2.7863\n",
      "Epoch 013/15 - Loss: 2.7856\n",
      "Epoch 014/15 - Loss: 2.7853\n",
      "Epoch 015/15 - Loss: 2.7850\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "_ = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_imgs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_imgs)\n",
    "\n",
    "        face_loss = loss_model(outputs)\n",
    "        loss = criterion(face_loss, batch_imgs, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # for i, tensor_img in enumerate(outputs):\n",
    "    #     img = img_tensor_to_numpy(tensor_img) * 255\n",
    "    #     img = img.astype(np.uint8)\n",
    "    #     Image.fromarray(img).save(f'./train_outputs/{_}_{i}.png')\n",
    "    # _ += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    # if epoch % 10 == 9:\n",
    "    #     print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\", end='\\t')\n",
    "    #     if (epoch + 1) % 20 == 0:\n",
    "    #         print()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "import validation_model\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "importlib.reload(validation_model)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils\n",
    "from validation_model import ValidationModel\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "GEN_PER_IMAGE = 10\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(100000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss()\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)\n",
    "\n",
    "for batch_imgs in dataloader:\n",
    "    outputs = model(batch_imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_model_output_image(outputs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.embed_raw_img_v1('bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh', 42).to(DEVICE) / 255\n",
    "_ = _.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "_ = _.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_ = model(_)\n",
    "utils.show_model_output_image(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = _ != 0\n",
    "i_[mask] = _[mask]\n",
    "\n",
    "utils.show_model_output_image(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.extract_text(i_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(960.)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(5, 3, 8, 8)\n",
    "t2 = torch.zeros(5, 3, 8, 8)\n",
    "\n",
    "mask = t1 > 0.5\n",
    "result = torch.zeros_like(t2)\n",
    "result[mask] = t2[mask]\n",
    "\n",
    "(result - t1).abs().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
