{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_model import ValidationModel\n",
    "from settings import *\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "from avapix.avapix_model import AvapixModel\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print (f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedFacesDataset(Dataset):\n",
    "    def __init__(self, num_samples) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.random_lengths = list(range(128))\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        curr_rand_len = random.choice(self.random_lengths)\n",
    "\n",
    "        curr_image_tensor = np.random.randint(0, 256, (8, 8, 3))\n",
    "        curr_image_tensor = torch.tensor(curr_image_tensor, device=DEVICE, dtype=torch.float)\n",
    "        output_img = utils.generate_input_v1(curr_image_tensor,\n",
    "                                             DEFAULT_RANDOM_SEED,\n",
    "                                             curr_rand_len)\n",
    "        output_img = output_img.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "\n",
    "        return output_img / 255\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(50_000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss(0.000001)\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/15 - Loss: 7.8098\n",
      "Epoch 002/15 - Loss: 4.0664\n",
      "Epoch 003/15 - Loss: 3.4634\n",
      "Epoch 004/15 - Loss: 3.1719\n",
      "Epoch 005/15 - Loss: 2.9691\n",
      "Epoch 006/15 - Loss: 2.8602\n",
      "Epoch 007/15 - Loss: 2.8128\n",
      "Epoch 008/15 - Loss: 2.7981\n",
      "Epoch 009/15 - Loss: 2.7920\n",
      "Epoch 010/15 - Loss: 2.7890\n",
      "Epoch 011/15 - Loss: 2.7873\n",
      "Epoch 012/15 - Loss: 2.7863\n",
      "Epoch 013/15 - Loss: 2.7856\n",
      "Epoch 014/15 - Loss: 2.7853\n",
      "Epoch 015/15 - Loss: 2.7850\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "_ = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_imgs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_imgs)\n",
    "\n",
    "        face_loss = loss_model(outputs)\n",
    "        loss = criterion(face_loss, batch_imgs, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # for i, tensor_img in enumerate(outputs):\n",
    "    #     img = img_tensor_to_numpy(tensor_img) * 255\n",
    "    #     img = img.astype(np.uint8)\n",
    "    #     Image.fromarray(img).save(f'./train_outputs/{_}_{i}.png')\n",
    "    # _ += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    # if epoch % 10 == 9:\n",
    "    #     print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\", end='\\t')\n",
    "    #     if (epoch + 1) % 20 == 0:\n",
    "    #         print()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "import validation_model\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "importlib.reload(validation_model)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils\n",
    "from validation_model import ValidationModel\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "GEN_PER_IMAGE = 10\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(100000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss()\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)\n",
    "\n",
    "for batch_imgs in dataloader:\n",
    "    outputs = model(batch_imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_model_output_image(outputs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.embed_raw_img_v1('bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh', 42).to(DEVICE) / 255\n",
    "_ = _.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "_ = _.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_ = model(_)\n",
    "utils.show_model_output_image(i_, 'after 30 epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(960.)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(5, 3, 8, 8)\n",
    "t2 = torch.zeros(5, 3, 8, 8)\n",
    "\n",
    "mask = t1 > 0.5\n",
    "result = torch.zeros_like(t2)\n",
    "result[mask] = t2[mask]\n",
    "\n",
    "(result - t1).abs().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
