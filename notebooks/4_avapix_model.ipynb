{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from validation_model import ValidationModel\n",
    "from settings import *\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "from avapix.avapix_model import AvapixModel\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "print (f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedFacesDataset(Dataset):\n",
    "    def __init__(self, num_samples) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.random_lengths = list(range(128))\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        curr_rand_len = random.choice(self.random_lengths)\n",
    "\n",
    "        curr_image_tensor = np.random.randint(0, 256, (8, 8, 3))\n",
    "        curr_image_tensor = torch.tensor(curr_image_tensor, device=DEVICE, dtype=torch.float)\n",
    "        output_img = utils.generate_input_v1(curr_image_tensor,\n",
    "                                             DEFAULT_RANDOM_SEED,\n",
    "                                             curr_rand_len)\n",
    "        output_img = output_img.reshape((8, 8, 3)).permute((2, 0, 1))\n",
    "\n",
    "        return output_img / 255\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(50_000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss(0.000001)\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=0.001,\n",
    "                             weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/5 - Loss: 2.4625\n",
      "Epoch 002/5 - Loss: 2.4618\n",
      "Epoch 003/5 - Loss: 2.4618\n",
      "Epoch 004/5 - Loss: 2.4618\n",
      "Epoch 005/5 - Loss: 2.4618\n",
      "Epoch 006/5 - Loss: 2.4617\n",
      "Epoch 007/5 - Loss: 2.4617\n",
      "Epoch 008/5 - Loss: 2.4617\n",
      "Epoch 009/5 - Loss: 2.4617\n",
      "Epoch 010/5 - Loss: 2.4617\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "_ = 0\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_imgs in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_imgs)\n",
    "\n",
    "        possibility_pred = loss_model(outputs)\n",
    "        \n",
    "        loss = criterion(possibility_pred)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # for i, tensor_img in enumerate(outputs):\n",
    "    #     img = img_tensor_to_numpy(tensor_img) * 255\n",
    "    #     img = img.astype(np.uint8)\n",
    "    #     Image.fromarray(img).save(f'./train_outputs/{_}_{i}.png')\n",
    "    # _ += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\")\n",
    "    # if epoch % 10 == 9:\n",
    "    #     print(f\"Epoch {epoch+1:03d}/{NUM_EPOCHS} - Loss: {avg_loss:.4f}\", end='\\t')\n",
    "    #     if (epoch + 1) % 20 == 0:\n",
    "    #         print()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Area\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import avapix.avapix_model\n",
    "import avapix.avapix_loss\n",
    "import avapix.avapix_utils\n",
    "import validation_model\n",
    "importlib.reload(avapix.avapix_model)\n",
    "importlib.reload(avapix.avapix_loss)\n",
    "importlib.reload(avapix.avapix_utils)\n",
    "importlib.reload(validation_model)\n",
    "from avapix.avapix_model import AvapixModel\n",
    "from avapix.avapix_loss import AvapixLoss\n",
    "import avapix.avapix_utils as utils\n",
    "from validation_model import ValidationModel\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "GEN_PER_IMAGE = 10\n",
    "\n",
    "image_list = os.listdir(VALID_FACE_DIR)\n",
    "image_list = [os.path.join(VALID_FACE_DIR, img_file) for img_file in image_list]\n",
    "\n",
    "dataset = EmbeddedFacesDataset(100000)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = AvapixModel()\n",
    "\n",
    "criterion = AvapixLoss()\n",
    "\n",
    "loss_model = ValidationModel()\n",
    "loss_model.load_state_dict(torch.load('./assets/validation_model.pth'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=LEARNING_RATE,\n",
    "                             weight_decay=1e-4)\n",
    "\n",
    "for batch_imgs in dataloader:\n",
    "    outputs = model(batch_imgs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.show_model_output_image(outputs[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = utils.embed_raw_img_v1('bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh', 42).to(DEVICE) / 255\n",
    "_ = _.reshape((3, 8, 8))\n",
    "_ = _.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_ = model(_)\n",
    "utils.show_model_output_image(i_, 'after 30 epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
